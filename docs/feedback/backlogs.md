# DSM Feedback: Backlog Proposals

**Project:** SQL Query Agent with Ollama
**DSM Version Used:** DSM 1.0 v1.1
**Author:** Alberto
**Date:** 2026-02-01

---

## Medium Priority

### Add research/literature review step to DSM_0
- **DSM Section:** DSM_0_START_HERE_Complete_Guide.md
- **Problem:** DSM_0 jumps straight from project setup to sprint planning. Projects that benefit from surveying state of the art (e.g., ML/AI projects) have no designated step for research before planning.
- **Proposed Solution:** Add an optional "Phase 0: Research" step between project setup and sprint planning in DSM_0, with guidance on when it applies (e.g., novel technique, unfamiliar domain, model selection needed).
- **Evidence:** We added a research phase that produced `docs/research/text_to_sql_state_of_art.md`, which fundamentally changed our architecture (added schema filtering, sqlglot validation, model selection). Without it, the plan would have been less informed.

### Add sprint-level plan template to PM Guidelines
- **DSM Section:** DSM_2.0_ProjectManagement_Guidelines_v2_v1.1.md
- **Problem:** Current templates (1-7) are oriented toward daily task breakdowns. Multi-sprint projects need a sprint-level planning template with phases, readiness checklists, and priority frameworks combined.
- **Proposed Solution:** Add a "Template 8: Sprint Plan" that combines elements from Templates 1 (task breakdown), 4 (prerequisites), and 7 (MUST/SHOULD/COULD) at the sprint level.
- **Evidence:** We had to manually combine three templates to create `docs/plans/sprint-1-plan.md`. A dedicated sprint template would save this synthesis effort.

### Clarify naming and relationship between Validation Tracker and Feedback files
- **DSM Section:** Section 6.4.5 + Appendix E.12
- **Problem:** The Validation Tracker (E.12) scores DSM sections, logs gaps, and proposes recommendations. The feedback backlogs file (6.4.5) also captures DSM gaps and improvement proposals. The overlap is significant and the naming ("validation" vs "feedback") doesn't clarify the distinction. Section 6.4.5 says they are "distinct" but in practice the tracker's recommendations directly feed into backlogs.
- **Proposed Solution:** Either (a) merge them into a single feedback system with a clear during-execution vs end-of-project distinction, or (b) rename for clarity: "DSM Usage Log" (runtime tracking) vs "DSM Feedback" (structured deliverables).
- **Evidence:** During project setup, we created both and immediately noticed the overlap, leading to confusion about where to log what.

### Add notebook collaboration protocol to Custom Instructions
- **DSM Section:** DSM_Custom_Instructions_v1.1.md + Section 6.2.5 + Section 7.1
- **Problem:** DSM 1.0 describes the cell-by-cell principle (Sections 6.2.5, 7.1) -- execute one cell, validate output, proceed. But neither the Custom Instructions nor these sections specify the actual interaction mechanics: how does the AI agent deliver cells to the user? The agent's default behavior was to write the .ipynb file directly, which bypasses the human-in-the-loop validation that cell-by-cell execution requires. The user had to explicitly correct this and state the protocol.
- **Proposed Solution:** Add a "Notebook Collaboration Protocol" to the Custom Instructions that specifies: (1) the agent provides cells as code/markdown blocks in the conversation, (2) the user pastes into the notebook and executes, (3) the user shares the output back, (4) the agent reads and validates output before providing the next cell, (5) the agent must not write .ipynb files directly during notebook work, (6) number code cells with a comment (`# Cell N`); markdown cells are structural headers and should NOT be numbered.
- **Evidence:**
  1. **(Phase 1)** The agent attempted to create `notebooks/01_sql_agent_exploration.ipynb` directly via file write. The user rejected this and stated the correct protocol. This is a fundamental interaction pattern for DSM 1.0 notebook projects that should be documented to prevent this misunderstanding on every new project.
  2. **(Phase 3)** After the protocol was established in the project's CLAUDE.md, the rule "Number each cell with a comment" was too broad. The agent added `# Cell 26` to a markdown cell (Phase 3 experiment header), which conflicted with the markdown heading hierarchy and added noise. The user removed it manually and corrected the rule. This shows the protocol needs granularity: code cells need numbering for output validation reference; markdown cells are structural and should not be numbered.

### Add docs/checkpoints/ to DSM_0 project structure template
- **DSM Section:** DSM_0_START_HERE_Complete_Guide.md (project structure) + PM Guidelines Template 5 (line 493)
- **Problem:** PM Guidelines Template 5 specifies `docs/checkpoints/` as the checkpoint location. DSM Section 6.4.1 requires milestone checkpoints. But the DSM_0 project structure template does not include a `checkpoints/` folder, so projects discover this gap only when they try to create their first checkpoint.
- **Proposed Solution:** Add `docs/checkpoints/` to the standard project directory structure in DSM_0, alongside `docs/plans/` and `docs/decisions/`.
- **Evidence:** We followed DSM_0 to set up our directory structure, then discovered we needed `docs/checkpoints/` when creating the setup milestone checkpoint.

### Link evaluation/experiment phases to DSM Appendix C.1 experiment framework
- **DSM Section:** DSM_1.0_Methodology_Appendices.md (C.1.3 Capability Experiment Template, C.1.5 Limitation Discovery Protocol, C.1.6 Experiment Artifact Organization) + DSM_1.0 Section 5.2.1 (Experiment Tracking) + DSM_4.0 Section 4.4 (Tests vs Capability Experiments)
- **Problem:** DSM has a comprehensive experiment framework (C.1.3-C.1.6) designed exactly for the kind of model comparison and evaluation we need in Phase 3. However, neither the sprint plan nor PLAN.md references these sections. The plan defines its own ad-hoc evaluation structure ("Evaluation Metrics" table, "Test Suite Structure") without using the existing templates. The AI agent's plan alignment assessment also missed this — it focused on technical adjustments (num_ctx, parsability definitions) but did not check whether DSM had a framework for running evaluations. The user had to explicitly direct the agent to consult DSM experiment sections.
- **Root cause (structural):** There is no trigger in the DSM workflow that says "before designing an evaluation/experiment phase, check Appendix C.1 for templates." The experiment framework is in Appendix C (Tier 2 practices) and Section 5.2.1, which are not referenced by DSM_0, the Custom Instructions, the sprint plan template, or the PM Guidelines. A project following the main flow (DSM_0 → Custom Instructions → sprint planning → execution) can easily miss these sections unless someone specifically knows to look for them.
- **What happened:** Our sprint plan defined Phase 3 as "Evaluation Framework" with activities designed from scratch. PLAN.md created a custom evaluation metrics table. When entering Phase 3, both the user and the AI agent prepared to build the evaluation harness without consulting DSM. Only when the user explicitly asked "Explore what DSM says about running experiments" did we discover that C.1.3 (Capability Experiment Template), C.1.5 (Limitation Discovery Protocol), C.1.6 (Artifact Organization), 5.2.1 (Experiment Tracking), and DSM 4.0 §4.4 (Tests vs Experiments) directly apply to our model comparison evaluation.
- **Additional factor:** The sprint plan calls it an "Evaluation Framework" — a term that doesn't map to either DSM concept of "tests" (function correctness) or "experiments" (capability validation). DSM 4.0 §4.4 makes this distinction explicit, but if you haven't read that section, the terminology gap means you don't make the connection.
- **Proposed Solution:** Three changes:
  1. **Sprint Plan Template:** For phases involving evaluation, comparison, or benchmarking, add a prompt: "Check DSM Appendix C.1 for experiment templates: C.1.3 (Capability Experiment), C.1.5 (Limitation Discovery), C.1.6 (Artifact Organization)."
  2. **Custom Instructions:** Add a "Phase-to-DSM-Section Mapping" that tells the AI agent which DSM sections to consult at each project stage. E.g., "Evaluation/experiment phases → Appendix C.1 + Section 5.2.1 + DSM 4.0 §4.4."
  3. **DSM_0 or PM Guidelines:** When defining sprint phases, include a reminder that evaluation activities should follow DSM's experiment framework rather than creating ad-hoc structures.
- **Evidence:** Phase 3 was about to be built without DSM experiment templates. The user caught this and redirected the work. Without that intervention, we would have created a non-standard evaluation structure that misses the Limitation Discovery Protocol (C.1.5), the artifact organization (C.1.6), and the structured experiment documentation (C.1.3) — all of which add significant value.
- **Impact:** Projects that include evaluation or benchmarking phases risk reinventing structures that DSM already provides. The existing experiment framework is well-designed but invisible to the standard project workflow.

### Add decision traceability and citation requirements to Custom Instructions and experiment framework
- **DSM Section:** DSM_Custom_Instructions_v1.1.md + C.1.3 (Capability Experiment Template) + Section 5.2.1 (Experiment Tracking)
- **Problem:** When the AI agent implements code that involves design choices (e.g., choosing between implementation approaches, adapting external concepts, making trade-off decisions), it delivers the code without documenting the rationale or citing external references. The DSM has DEC-### records for project-level decisions, but no equivalent guidance for implementation-level decisions within experiments or notebook cells. The C.1.3 experiment template has a References section but only for the experiment overall, not for individual design choices within the experiment's implementation. Section 5.2.1 covers parameter logging but not decision rationale.
- **What happened:** Cell 28 (evaluation harness) contained three design decisions: (1) using `graph.stream()` over two alternatives for raw SQL capture, (2) a flexible comparison strategy adapting the Spider benchmark's EX metric, (3) an error categorization priority hierarchy. It also referenced external concepts (Spider EX metric, LangGraph streaming API, sqlglot). The agent delivered the code without documenting any of this. The user had to explicitly request documentation of decisions and a citations log. This is the third instance in Phase 3 where the user caught a documentation gap (after the experiment framework and cell numbering incidents), suggesting a systematic pattern rather than isolated oversights.
- **Root cause:** The Custom Instructions tell the agent *what* to build but not *how to document the reasoning*. There is no rule that says "when you make a design choice during implementation, document the alternatives considered and the rationale for the choice." The DSM's experiment framework (C.1.3) assumes documentation happens at the experiment level, not at the individual implementation step level.
- **Proposed Solution:** Two changes:
  1. **Custom Instructions:** Add a rule: "When implementing code that involves design choices (alternative approaches considered, external concepts adapted, non-obvious trade-offs), document the decision rationale before or alongside the implementation. Maintain a citations log for external benchmarks, APIs, or research referenced in the code."
  2. **C.1.3 Experiment Template:** Add a "Design Decisions" subsection (for implementation-level decisions, distinct from project-level DEC-### records) and expand the References section to explicitly require citations for external tools, benchmarks, and APIs used in the experiment code.
- **Evidence:** Cell 28 was delivered as pure code. After user intervention, three design decisions (ED-1 through ED-3) and a full citations section were documented in the EXP-001 README. The documentation added significant traceability value — without it, future readers would not understand *why* `graph.stream()` was chosen or how the comparison function was designed.
- **Impact:** Implementation decisions made without documentation are invisible to future readers (including the same developer returning later). For experiments specifically, undocumented design choices make results harder to reproduce and findings harder to validate.

### Define intra-sprint documentation cadence in PM Guidelines and sprint plan templates
- **DSM Section:** DSM_2.0_ProjectManagement_Guidelines_v2_v1.1.md (Template 5: Checkpoints) + Section 6.4.1 (Milestone Checkpoints)
- **Problem:** DSM defines documentation checkpoints at **sprint boundaries** (CLAUDE.md: "Follow the sprint boundary checklist: checkpoint, feedback files, decision log, blog entry"). But sprints with multiple phases have natural internal milestones (e.g., Phase 1 → Phase 2 transition) where documentation should also happen. The DSM is silent about this intra-sprint cadence, and neither the PM Guidelines nor the sprint plan template prompts the user to include documentation steps at phase boundaries.
- **What happened:** Our setup checkpoint explicitly stated *"Next checkpoint: After Sprint 1 Phase 1 completion."* Our sprint plan defined Phase 1 with a "Readiness for Phase 2" checklist — but the checklist was purely technical (Ollama responds, database loads, models pulled). No documentation step was included. As a result, we completed Phase 1, moved straight into Phase 2, built all 5 agent nodes, and only then realized we hadn't documented Phase 1 completion. The checkpoint, methodology update, decision record (DEC-003), and blog materials were all created retroactively in the middle of Phase 2.
- **Root cause (dual):** (1) The sprint plan didn't include documentation as a phase activity because the template doesn't prompt for it. (2) The DSM only mandates checkpoints at sprint boundaries, not at phase boundaries, so there was no trigger to pause and document.
- **Proposed Solution:** Two changes:
  1. **PM Guidelines — Sprint Plan Template:** Add a standard "Phase Boundary Checklist" section to each phase definition that includes: `[ ] Update methodology.md`, `[ ] Create checkpoint if significant milestone`, `[ ] Log any new decisions to docs/decisions/`, `[ ] Update blog materials`. This makes documentation a planned activity, not an afterthought.
  2. **PM Guidelines — Section 6.4.1:** Clarify that checkpoints should occur at significant milestones within a sprint, not only at sprint boundaries. Add guidance: "If a sprint has multiple phases, consider a lightweight checkpoint at each phase transition."
- **Evidence:** Phase 1 completed (Cells 1-7), Phase 2 nodes built (Cells 8-16), but documentation was only triggered by the user noticing the gap mid-Phase 2. The retroactive checkpoint covered both phases at once, losing the clean Phase 1 → Phase 2 transition record. A planned documentation step would have caught this.
- **Impact:** Without intra-sprint documentation cadence, progress is either undocumented until sprint end (losing detail) or documented retroactively (losing accuracy). Both reduce the value of the feedback loop that DSM is designed to create.

### Add notebook-to-script transition guidance to sprint plan template and notebook protocol
- **DSM Section:** DSM_2.0_ProjectManagement_Guidelines_v2_v1.1.md (Sprint Plan Template) + DSM_Custom_Instructions_v1.1.md (Notebook Protocol)
- **Problem:** The DSM Notebook Collaboration Protocol defines how to work *within* notebooks (cell-by-cell, validate output, proceed) but does not define when to *transition out* of notebooks into scripts. The sprint plan template defines phases by objective but not by execution mode (notebook vs script). Projects that start in notebooks for exploration naturally reach a point where the work shifts from interactive design to long-running computation — but nothing in the DSM workflow triggers this transition. The result is that evaluation harnesses, batch runners, and other execution-heavy code get written as notebook cells, inheriting all notebook limitations (no checkpointing, namespace fragility, no CLI execution, non-reproducibility).
- **What happened:** Phase 3 of our sprint was defined as "Evaluation Framework" — a phase that contains both design work (test suite definition, ground truth computation) and execution work (evaluation harness, 14-query model comparison runs). All of this was initially implemented in notebook cells (28-29). The evaluation harness was ~150 lines with complex logic; the model run took ~7 minutes. A namespace bug (`graph` vs `agent` — Cell 28b) was caused by notebook variable scope and would not have occurred in a standalone script. After the first run, we retroactively extracted to `scripts/eval_harness.py` and `scripts/run_experiment.py`. The user identified that the transition should have happened *before* Cell 28, not after: "In the future we should have identified moving to working in scripts in Cell 28."
- **Proposed Solution:** Two changes:
  1. **Sprint Plan Template:** Add an "Execution Mode" indicator to each phase definition. Phases should be tagged as `notebook` (interactive exploration), `script` (batch computation), or `both` (design in notebook, execution via script). This makes the transition point explicit during planning, not discovered during execution.
  2. **Custom Instructions / Notebook Protocol:** Add a transition rule: "When the next step involves long-running computation (>2 minutes), batch processing, or generating results that must be persisted independently, extract the code to a script. The notebook should import from or call the script, or load its JSON/CSV output — not replicate the computation inline." Also add: "Reusable functions (>50 lines) with complex logic should be extracted to Python modules in `scripts/` and imported into the notebook, rather than defined in notebook cells."
- **Evidence:** Cell 28 (evaluation harness) and Cell 29 (model run) were written, debugged, and executed in the notebook. Cell 28b was a namespace fix that scripts would avoid. The retroactive extraction to scripts was successful but the first model's evaluation had already been done in the less-robust notebook environment. The second model (llama3.1:8b) will run via the extracted script, demonstrating the correct workflow.
- **Impact:** Projects with evaluation, benchmarking, or batch-processing phases will repeatedly hit this transition gap. The notebook protocol is thorough for interactive work but creates a false sense that *all* sprint work should happen in notebooks. Making the transition point explicit in the sprint plan prevents this class of problems.

### Add README update to sprint boundary checklist
- **DSM Section:** CLAUDE.md template (sprint boundary checklist) + DSM_Custom_Instructions_v1.1.md + DSM_2.0_ProjectManagement_Guidelines_v2_v1.1.md (Template 5: Checkpoints)
- **Problem:** The sprint boundary checklist specifies: "checkpoint, feedback files, decision log, blog entry." The repository README — the project's primary external-facing document — is not included. At sprint boundaries, the README typically needs updates to project status, current results, project structure, and recommendations. Without an explicit checklist item, the README becomes stale: our README showed Phase 2 results but not Phase 3 evaluation results, the model recommendation, or the updated project structure (scripts, experiment artifacts).
- **Proposed Solution:** Add "Update repository README" as a checklist item in: (1) the sprint boundary checklist in CLAUDE.md template / Custom Instructions, (2) PM Guidelines Template 5 (Checkpoints), under a "Documentation Updates" section. The README update should cover: project status/progress markers, latest results summary, updated project structure (if new folders/files were added during the sprint), and any changed recommendations or decisions.
- **Evidence:** Sprint 1 completion required updating the README with: Phase 3 status (complete), EXP-001 evaluation results, model recommendation (llama3.1:8b), updated project structure (`scripts/`, `data/experiments/`), and recommended models table revision. This was caught by the user noticing the gap, not by the checklist. All other sprint boundary documentation was triggered by the checklist — the README was the only artifact missed.

### Separate blog materials from DSM feedback in Section 6.4.5 file organization
- **DSM Section:** Section 6.4.5 (Feedback Deliverables) + DSM_0_START_HERE_Complete_Guide.md (project structure)
- **Problem:** DSM Section 6.4.5 groups three "feedback deliverables": methodology assessment, backlog proposals, and blog materials. This grouping is logical by timing (all produced at sprint boundaries) but conflates two different categories: (1) DSM methodology feedback (methodology.md, backlogs.md) and (2) project deliverable working files (blog materials). When following the DSM, the blog tracker was placed in `docs/feedback/blog.md` alongside the other two, but blog materials (narrative angles, screenshots, draft structures) are project output, not DSM feedback. They naturally belong in `docs/blog/` with other blog content (images, drafts, published posts).
- **Proposed Solution:** In Section 6.4.5, distinguish between: (a) **DSM feedback files** → `docs/feedback/` (methodology.md, backlogs.md), and (b) **project deliverable trackers** → their natural project locations (blog → `docs/blog/`, presentations → `docs/presentations/`, etc.). The sprint boundary checklist should reference both. Alternatively, Section 6.4.5 could note: "Blog materials are tracked per Section 2.5.6 and should be placed in `docs/blog/`, not in the feedback directory."
- **Evidence:** Blog materials file was created in `docs/feedback/blog.md` during project setup, following the Section 6.4.5 grouping. Had to be moved to `docs/blog/blog.md` at sprint boundary when the misplacement became apparent. The `docs/blog/` directory already existed (per DSM_0 project structure) but the file was placed elsewhere due to the feedback grouping.

### Add blog artifact naming convention to Section 2.5.6
- **DSM Section:** Section 2.5.6 (Blog Deliverable Process) + DSM_0_START_HERE_Complete_Guide.md (project structure)
- **Problem:** Section 2.5.6 defines a 6-step blog process (collect → outline → draft → review → finalize → publish) but does not specify a naming convention for the artifacts produced at each step. The `docs/blog/` folder will contain multiple artifact types — collected materials, drafted posts, and final publications — potentially across multiple sprints/phases. Without a naming convention, the first file gets a generic name (e.g., `blog.md`) that blocks subsequent files and creates ambiguity about what each file represents.
- **Proposed Solution:** Add a naming convention to Section 2.5.6 or the DSM_0 project structure template:
  - `blog-materials-{scope}.md` — Raw materials collected during work (step 1)
  - `blog-{scope}.md` — Drafted blog post (steps 2-4)
  - `post-{scope}.md` — Final publication-ready post (steps 5-6)
  - Where `{scope}` follows the project's sprint/phase convention (e.g., `s01`, `s01-phase3`, `final`)
  - Supporting assets (screenshots, diagrams) can use any name but should be referenced from the materials file
- **Evidence:** The blog file was initially named `blog.md`, which gave no indication of its role (materials tracking) or scope (Sprint 1). When it came time to think about the actual blog draft and final post, the name was already taken. Renamed to `blog-materials-s01.md` to make room for `blog-s01.md` (draft) and `post-s01.md` (final) in the same directory.

---

## Low Priority

(To be populated as project progresses)
